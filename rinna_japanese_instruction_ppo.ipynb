{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chat experiment using rinna/japanese-gpt-neox-3.6b-instruction-ppo"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers accelerate sentencepiece langchain"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "model_repo = 'rinna/japanese-gpt-neox-3.6b-instruction-ppo'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_repo, use_fast=False)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_repo, device_map='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models.base import BaseChatModel\n",
    "from langchain.schema import BaseMessage, AIMessage, HumanMessage, SystemMessage, ChatResult, ChatGeneration\n",
    "from typing import Optional, List\n",
    "import transformers\n",
    "\n",
    "class RinnaChat(BaseChatModel):\n",
    "  tokenizer: transformers.models.t5.tokenization_t5.T5Tokenizer\n",
    "  model: transformers.models.gpt_neox.modeling_gpt_neox.GPTNeoXForCausalLM\n",
    "  \n",
    "  def get_prompt(self, messages: List[BaseMessage])->str:\n",
    "    prompt = \"\"\n",
    "    for i, message in enumerate(messages):\n",
    "      if type(message)==HumanMessage:\n",
    "        prompt += f\"ユーザー: {message.content}<NL>\"\n",
    "      else:\n",
    "        prompt += f\"システム: {message.content}<NL>\"\n",
    "    prompt += \"システム: \"\n",
    "    return prompt\n",
    "  \n",
    "  def _generate(self, messages: List[BaseMessage], stop: Optional[List[str]]=None)->ChatResult:\n",
    "    prompt = self.get_prompt(messages)\n",
    "    token_ids = self.tokenizer.encode(prompt, add_special_tokens=False, return_tensors='pt')\n",
    "    with torch.no_grad():\n",
    "      output_ids = self.model.generate(\n",
    "          token_ids.to(model.device),\n",
    "          do_sample=True,\n",
    "          max_new_tokens=128,\n",
    "          temperature=0.8,\n",
    "          repetition_penalty=1.1,\n",
    "          pad_token_id = tokenizer.pad_token_id,\n",
    "          bos_token_id = tokenizer.bos_token_id,\n",
    "          eos_token_id = tokenizer.eos_token_id,\n",
    "      )\n",
    "    output = self.tokenizer.decode(output_ids.tolist()[0][token_ids.size(1):], skip_special_tokens=True)\n",
    "    output = output.replace(\"<NL>\", \"\\n\").strip()\n",
    "    if stop is not None:\n",
    "      for stop_word in stop:\n",
    "        if output.find(stop_word) != -1:\n",
    "          output = output[:output.find(stop_word)]\n",
    "    ai_message = AIMessage(content=output.strip())\n",
    "    chat_result = ChatResult(generations = [ChatGeneration(message=ai_message)])\n",
    "    return chat_result\n",
    "\n",
    "  def _agenerate():\n",
    "    return None\n",
    "\n",
    "  def _llm_type():\n",
    "    return \"rinna\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example of simple chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat([\n",
    "    HumanMessage(content=\"こんにちは\")\n",
    "])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example of chat loop using ConversationChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts.prompt import PromptTemplate\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "conversation = ConversationChain(\n",
    "    llm=chat,\n",
    "    prompt=PromptTemplate(\n",
    "        input_variables=['history', 'input'],\n",
    "        template=\"{history}<NL>ユーザー: {input}<NL>システム: \"\n",
    "    ),\n",
    "    memory=ConversationBufferMemory(ai_prefix='システム', human_prefix=\"ユーザー\"),\n",
    "    # verbose=True\n",
    ")\n",
    "\n",
    "while True:\n",
    "  inp = input('ユーザー: ')\n",
    "  print(\"システム: \", end='')\n",
    "  print(conversation.run(inp))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
